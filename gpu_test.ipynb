{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numba as nb\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_kernel_bias(num_inp_channels, kernel_size, num_kernels,mean=0,std=0.01):\n",
    "    shape = [num_inp_channels, kernel_size, kernel_size, num_kernels]\n",
    "    weights = std*np.random.randn(*shape) + mean\n",
    "    # weights/=np.sqrt(num_inp_channels)\n",
    "    bias = std*np.random.randn(1,num_kernels) + mean\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0,b0=init_kernel_bias(num_inp_channels=32,kernel_size=3,num_kernels=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=np.random.randn(128,64,64,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inp[batches,row,col,d],w0(d,ksz,ksz,num_ker),b0[1,num_ker],stride[row,col]\n",
    "padding=0\n",
    "stride=[1,1]\n",
    "ipp=inp.transpose(0,3,1,2)  #ipp[batches,d,row,col]\n",
    "output=[]\n",
    "ksz=w0.shape[1]\n",
    "num_ker=w0.shape[3]\n",
    "if not padding: #take care of padding in backprop too\n",
    "    padding=(ksz-1)//2  #currently don't give 'even' ksz\n",
    "out_row,out_col=((ipp.shape[2]-ksz+2*padding)//stride[0]+1),((ipp.shape[3]-ksz+2*padding)//stride[1]+1)\n",
    "batches,d,row,col=ipp.shape\n",
    "row+=2*padding\n",
    "col+=2*padding\n",
    "padded=np.zeros((batches,d,row,col))\n",
    "padded[:,:,padding:-padding,padding:-padding]=ipp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "window=(np.arange(ksz)[:,None]*row+np.arange(ksz)).ravel()+np.arange(d)[:,None]*row*col\n",
    "slider=(np.arange(out_row*stride[0])[:,None]*row+np.arange(out_col*stride[1]))\n",
    "ind = window.ravel()+slider[::stride[0],::stride[1]].ravel()[:,None]\n",
    "# bind= np.arange(batches)[:,None]*d*row*col+ind.ravel()\n",
    "kern = w0.reshape(-1,num_ker)\n",
    "# output=(np.dot(np.take(padded, bind).reshape(-1,d*ksz*ksz), kern)).reshape(batches,out_row,out_col,num_ker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    }
   ],
   "source": [
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weakproxy at 0x7f50917a3c28 to Device at 0x7f503f854a90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.select_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Initialize the data arrays\n",
    "A = np.asfortranarray(np.random.randn(32*2, 32*3))\n",
    "B = np.asfortranarray(np.random.randn(3*32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.7 ms, sys: 4.09 ms, total: 8.79 ms\n",
      "Wall time: 7.51 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Copy the arrays to the device\n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(B)\n",
    "\n",
    "# Allocate memory on the device for the result\n",
    "C_global_mem = cuda.device_array((32*2, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 1 µs, total: 11 µs\n",
      "Wall time: 13.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Configure the blocks\n",
    "threadsperblock = (16,16)\n",
    "blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[0]))\n",
    "blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[1]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blockspergrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0859375"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A.nbytes+B.nbytes+np.dot(A,B).nbytes)/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 189 ms, sys: 272 µs, total: 190 ms\n",
      "Wall time: 188 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 32)\n",
      "CPU times: user 710 µs, sys: 66 µs, total: 776 µs\n",
      "Wall time: 607 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C = C_global_mem.copy_to_host()\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 762 µs, sys: 0 ns, total: 762 µs\n",
      "Wall time: 515 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "np.allclose(C,np.dot(A,B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.21309449,   2.89234672,  28.20324537, ..., -14.19437499,\n",
       "          3.97631953, -19.85510025],\n",
       "       [  7.79791636,   8.69099402, -19.05600696, ...,   5.10313222,\n",
       "         23.15987213, -14.31547735],\n",
       "       [ -5.62363989, -11.57985623,  -0.81119961, ...,   6.15165425,\n",
       "          6.46526696, -15.27875407],\n",
       "       ...,\n",
       "       [-15.96582059,  -1.89555769,   4.86732631, ...,  17.79446482,\n",
       "          4.05454805,  20.05742225],\n",
       "       [  6.26804982,  15.04490977,   0.58337399, ...,  -8.24276073,\n",
       "        -22.72991009, -23.79241361],\n",
       "       [  5.1795037 ,  -9.69664089,  -9.41470239, ...,   3.58212059,\n",
       "          0.82304345,   5.880077  ]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.21309448,   2.89234644,  28.20324633, ..., -14.19437491,\n",
       "          3.97631955, -19.85510151],\n",
       "       [  7.79791662,   8.69099371, -19.05600668, ...,   5.10313206,\n",
       "         23.15987269, -14.31547733],\n",
       "       [ -5.62363966, -11.57985586,  -0.81119891, ...,   6.15165372,\n",
       "          6.46526699, -15.27875432],\n",
       "       ...,\n",
       "       [-15.96581962,  -1.89555792,   4.86732652, ...,  17.79446452,\n",
       "          4.05454818,  20.0574224 ],\n",
       "       [  6.2680493 ,  15.04490923,   0.58337351, ...,  -8.24276069,\n",
       "        -22.72991012, -23.79241307],\n",
       "       [  5.17950452,  -9.69664087,  -9.41470208, ...,   3.58212069,\n",
       "          0.82304406,   5.88007726]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n",
      "[[8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      " [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]]\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "@cuda.jit\n",
    "def my_kernel_2D(io_array):\n",
    "    x, y = cuda.grid(2)\n",
    "    if x<io_array.shape[0] and y<io_array.shape[1]:\n",
    "        io_array[x,y]*=8\n",
    "\n",
    "data = np.ones((16, 16))\n",
    "data_glob=cuda.to_device(data)\n",
    "threadsperblock = (32, 32)\n",
    "blockspergrid_x = math.ceil(data.shape[0] / threadsperblock[0])\n",
    "blockspergrid_y = math.ceil(data.shape[1] / threadsperblock[1])\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "my_kernel_2D[blockspergrid, threadsperblock](data_glob)\n",
    "print(data_glob.copy_to_host())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.79 s, sys: 163 ms, total: 4.96 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output=np.empty((batches,out_row*out_col,num_ker))\n",
    "for i,img in enumerate(padded):      #img[d,row,col]\n",
    "    # windows(out_row*out_col, ksz*ksz*d) . kernels(d*ksz*ksz,num_ker)\n",
    "    output[i]=np.dot(img.take(ind), kern)\n",
    "output+=b0\n",
    "ans2=output.reshape(batches,out_row,out_col,num_ker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 96), (96, 32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape,B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPB=16\n",
    "@cuda.jit\n",
    "def fast_matmul(A,B,C):\n",
    "    sA=cuda.shared.array(shape=(TPB,TPB),dtype=nb.float32)\n",
    "    sB=cuda.shared.array(shape=(TPB,TPB),dtype=nb.float32)\n",
    "    x,y=cuda.grid(2)\n",
    "    tx=cuda.threadIdx.x\n",
    "    ty=cuda.threadIdx.y\n",
    "    if x>=C.shape[0] and y>=C.shape[1]:\n",
    "        return\n",
    "    tmp=0\n",
    "    for i in range(A.shape[1]//TPB):  # for number of blocks\n",
    "        sA[tx,ty]=A[x,ty+i*TPB]       # preLoad data into shared memory\n",
    "        sB[tx,ty]=B[tx+i*TPB,y]\n",
    "        \n",
    "        cuda.syncthreads()            # wait for loading\n",
    "        for j in range(TPB):\n",
    "            tmp+=sA[tx,j]*sB[j,ty]\n",
    "            cuda.syncthreads()            # wait for computation\n",
    "    C[x,y]=tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 252 ms, sys: 13 ms, total: 265 ms\n",
      "Wall time: 269 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fast_matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 32)\n",
      "CPU times: user 358 µs, sys: 269 µs, total: 627 µs\n",
      "Wall time: 453 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C = C_global_mem.copy_to_host()\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudlib=ctypes.CDLL('libcublas.so')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array(np.arange(N ** 2, dtype=np.float32).reshape(N, N), order='F')\n",
    "B = np.array(np.arange(N) + 10, dtype=A.dtype, order='F')\n",
    "D = np.zeros_like(A, order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument 7: <class 'TypeError'>: Don't know how to convert parameter 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-59921d7f4cec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcudlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcublasZgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'N'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'N'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_global_mem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m: argument 7: <class 'TypeError'>: Don't know how to convert parameter 7"
     ]
    }
   ],
   "source": [
    "cudlib.cublasZgemm('N', 'N', N, N, N, 1, A_global_mem, np.diag(B), 1.0, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'accelerate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-da2e85391556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'accelerate'"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from accelerate.cuda.blas import Blas\n",
    "\n",
    "\n",
    "N = 128     # no. of rows/cols\n",
    "\n",
    "\n",
    "def gemm_v1():\n",
    "    '''\n",
    "    Note that all arrays are in Fortran order.\n",
    "    '''\n",
    "    print(\"Version 1\".center(80, '='))\n",
    "    # Prepare arrays for input\n",
    "    A = np.array(np.arange(N ** 2, dtype=np.float32).reshape(N, N), order='F')\n",
    "    B = np.array(np.arange(N) + 10, dtype=A.dtype, order='F')\n",
    "    D = np.zeros_like(A, order='F')\n",
    "\n",
    "    # NumPy\n",
    "    start = timer()\n",
    "    E = np.dot(A, np.diag(B))\n",
    "    numpy_time = timer() - start\n",
    "    print(\"Numpy took %f seconds\" % numpy_time)\n",
    "\n",
    "    # cuBLAS\n",
    "    blas = Blas()\n",
    "\n",
    "    start = timer()\n",
    "    blas.gemm('N', 'N', N, N, N, 1.0, A, np.diag(B), 1.0, D)\n",
    "    cuda_time = timer() - start\n",
    "\n",
    "    print(\"CUBLAS took %f seconds\" % cuda_time)\n",
    "    diff = np.abs(D - E)\n",
    "    print(\"Maximum error %f\" % np.max(diff))\n",
    "\n",
    "\n",
    "def gemm_v2():\n",
    "    \"\"\"\n",
    "    Let GEMM transpose the input matrices so that they can be in C order,\n",
    "    originally.  Note that the output matrix is still in Fortran array.\n",
    "    The string arguments in gemm tells it to apply transformation on the input\n",
    "    matrices.\n",
    "    See argument description in:\n",
    "        http://docs.continuum.io/accelerate/cublas#blas-level-2\n",
    "    \"\"\"\n",
    "    print(\"Version 2\".center(80, '='))\n",
    "    # Prepare arrays for input\n",
    "    A = np.array(np.arange(N ** 2, dtype=np.float32).reshape(N, N))\n",
    "    B = np.array(np.arange(N) + 10, dtype=A.dtype)\n",
    "    D = np.zeros_like(A, order='F')\n",
    "\n",
    "    # NumPy\n",
    "    start = timer()\n",
    "    E = np.dot(A, np.diag(B))\n",
    "    numpy_time = timer() - start\n",
    "    print(\"Numpy took %f seconds\" % numpy_time)\n",
    "\n",
    "    # cuBLAS\n",
    "    blas = Blas()\n",
    "\n",
    "    start = timer()\n",
    "    blas.gemm('T', 'T', N, N, N, 1.0, A, np.diag(B), 1.0, D)\n",
    "    cuda_time = timer() - start\n",
    "\n",
    "    print(\"CUBLAS took %f seconds\" % cuda_time)\n",
    "    diff = np.abs(D - E)\n",
    "    print(\"Maximum error %f\" % np.max(diff))\n",
    "\n",
    "\n",
    "def main():\n",
    "    gemm_v1()\n",
    "    gemm_v2()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
