{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from mnist import MNIST\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset....\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset....\")\n",
    "mndata = MNIST('../mnist_dataset')\n",
    "X_train, y_train = mndata.load_training()\n",
    "X_train = (mndata.process_images_to_numpy(X_train)/255)\n",
    "X_test, y_test = mndata.load_testing()\n",
    "X_test = (mndata.process_images_to_numpy(X_test)/255)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    y_inp=np.zeros((len(labels),10))\n",
    "    for ind,val in enumerate(labels):\n",
    "        y_inp[ind][val]=1\n",
    "    return y_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen(st=0,size=20,validate=False):\n",
    "    st=st%60000\n",
    "    if validate:\n",
    "        X=X_test[st:st+size].reshape(size,784)\n",
    "        labels=y_test[st:st+size]\n",
    "    else:\n",
    "        X=X_train[st:st+size].reshape(size,784)\n",
    "        labels=y_train[st:st+size]\n",
    "    y=one_hot_encode(labels)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 151\n",
      "Seed: 408\n"
     ]
    }
   ],
   "source": [
    "from nnet.network import Sequential,layers\n",
    "from nnet.layers import conv2d,max_pool,flatten,dense,dropout,Activation\n",
    "from nnet import functions\n",
    "from nnet import optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(dense(128,input_shape=(784),activation=functions.sigmoid))\n",
    "model.add(dense(32,activation=functions.sigmoid))\n",
    "model.add(dense(10,activation=functions.sigmoid))\n",
    "# model.add(Activation(functions.sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽⎽\n",
      "Layer (type)               Output Shape             Activation        Param #\n",
      "==========================================================================================\n",
      "0 input_layer(InputLayer) (None, 784)                echo             0\n",
      "__________________________________________________________________________________________\n",
      "1 dense(dense)            (None, 128)                sigmoid          100480\n",
      "__________________________________________________________________________________________\n",
      "2 dense(dense)            (None, 32)                 sigmoid          4128\n",
      "__________________________________________________________________________________________\n",
      "3 dense(dense)            (None, 10)                 sigmoid          330\n",
      "==========================================================================================\n",
      "Total Params: 104,938\n",
      "Trainable Params: 104,938\n",
      "Non-trainable Params: 0\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.iterative,loss=functions.mean_squared_error,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "btsz=2\n",
    "inp,y_inp=batch_gen(0,size=btsz)\n",
    "logits=model.fit(inp,y_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inp=inp\n",
    "labels=y_inp\n",
    "for obj in model.sequence:\n",
    "\tX_inp=obj.forward(X_inp)\n",
    "err=model.del_loss(X_inp,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5020977 ,  0.4387907 ,  0.4360888 ,  0.44107332,  0.45350362,\n",
       "        -0.50795789,  0.45003523,  0.43680118,  0.45020728,  0.44937507],\n",
       "       [-0.49790969,  0.43879869,  0.43610379,  0.4410947 ,  0.4534963 ,\n",
       "         0.49204316,  0.45006011,  0.43680938,  0.45020762,  0.44938554]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(inp)-y_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=model.lenseq_m1\n",
    "for obj in model.sequence[::-1]:\n",
    "\terr=obj.backprop(err,layer=i)\n",
    "\ti-=1\n",
    "model.optimizer(model.sequence,model.learning_rate,model.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4, 4]), array([5, 0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.argmax(axis=1),y_inp.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_sum() got an unexpected keyword argument 'keedims'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b3c58f3d4c82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeedims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: _sum() got an unexpected keyword argument 'keedims'"
     ]
    }
   ],
   "source": [
    "logits.sum(axis=0,keedims=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    vdsz=2000\n",
    "    dvme=10000//vdsz\n",
    "    acc=0\n",
    "    cross_entropy_loss=0\n",
    "    for smpl in range(0,10000,vdsz):\n",
    "        print(\"\\rCalculating Validation acc...\",smpl//vdsz+1,end='')\n",
    "        inp,y_inp=batch_gen(smpl,size=vdsz,validate=True)\n",
    "        logits=model.predict(inp)\n",
    "        ans=logits.argmax(axis=1)\n",
    "        cor=y_inp.argmax(axis=1)\n",
    "        acc+=100*(ans==cor).mean()\n",
    "        cross_entropy_loss+=model.loss(logits,labels=y_inp).mean()*10\n",
    "    print(\"\\rValidation Acc: {:.3f} %        Val loss: {:.8f}\".format(acc/dvme,cross_entropy_loss/dvme))\n",
    "    logits=model.predict(inp[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    st=0\n",
    "    btsz=16\n",
    "    acc_tn=0\n",
    "    _cc=0\n",
    "    loss_tn=0\n",
    "    while st<=60000:\n",
    "        sam_tm=time()\n",
    "        perc=st/600\n",
    "        ck=np.random.randint(0,60000-btsz)\n",
    "        inp,y_inp=batch_gen(ck,size=btsz)\n",
    "        logits=model.fit(inp,y_inp)\n",
    "        ans=logits.argmax(axis=1)\n",
    "        cor=y_inp.argmax(axis=1)\n",
    "        acc=100*(ans==cor).mean()\n",
    "        cross_entropy_loss=model.loss(logits=logits,labels=y_inp).mean()*10\n",
    "        acc_tn+=acc\n",
    "        _cc+=1\n",
    "        loss_tn+=cross_entropy_loss\n",
    "        acc=acc_tn/_cc\n",
    "        loss_=loss_tn/_cc\n",
    "        if acc>=98:\n",
    "            model.learning_rate=1e-5\n",
    "        elif acc>=97:\n",
    "            model.learning_rate=1e-4\n",
    "#         elif acc>=95:\n",
    "#             model.learning_rate=1e-4\n",
    "        sam_tm=time()-sam_tm\n",
    "        rem_sam=(60000-st)/btsz\n",
    "        eta=int(rem_sam*sam_tm)\n",
    "        print(\"\\rProgress: {:.2f} %    Acc: {:.3f} %    loss: {:.6f}     Sample time: {:.3f}s    ETA: {}:{}s    .\".format(perc,acc,loss_,sam_tm,eta//60,eta%60),end='')\n",
    "        st+=btsz\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 / 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress: 0.00 %    Acc: 12.500 %    loss: 1.242929     Sample time: 0.024s    ETA: 1:30s    .\r",
      "Progress: 0.03 %    Acc: 9.375 %    loss: 0.889794     Sample time: 0.002s    ETA: 0:7s    .\r",
      "Progress: 0.05 %    Acc: 10.417 %    loss: 0.745629     Sample time: 0.002s    ETA: 0:5s    .\r",
      "Progress: 0.08 %    Acc: 9.375 %    loss: 0.676027     Sample time: 0.003s    ETA: 0:9s    .\r",
      "Progress: 0.11 %    Acc: 10.000 %    loss: 0.630418     Sample time: 0.002s    ETA: 0:9s    .\r",
      "Progress: 0.13 %    Acc: 8.333 %    loss: 0.600234     Sample time: 0.002s    ETA: 0:5s    .\r",
      "Progress: 0.16 %    Acc: 7.143 %    loss: 0.581525     Sample time: 0.002s    ETA: 0:8s    .\r",
      "Progress: 0.19 %    Acc: 7.031 %    loss: 0.565341     Sample time: 0.002s    ETA: 0:6s    .\r",
      "Progress: 0.21 %    Acc: 6.944 %    loss: 0.553035     Sample time: 0.003s    ETA: 0:10s    .\r",
      "Progress: 0.24 %    Acc: 6.250 %    loss: 0.542489     Sample time: 0.003s    ETA: 0:11s    .\r",
      "Progress: 0.27 %    Acc: 6.250 %    loss: 0.534399     Sample time: 0.003s    ETA: 0:13s    .\r",
      "Progress: 0.29 %    Acc: 6.250 %    loss: 0.526979     Sample time: 0.002s    ETA: 0:7s    .\r",
      "Progress: 0.32 %    Acc: 6.731 %    loss: 0.521126     Sample time: 0.003s    ETA: 0:12s    .\r",
      "Progress: 0.35 %    Acc: 6.696 %    loss: 0.516501     Sample time: 0.004s    ETA: 0:14s    .\r",
      "Progress: 0.37 %    Acc: 6.667 %    loss: 0.512463     Sample time: 0.003s    ETA: 0:9s    .\r",
      "Progress: 0.40 %    Acc: 7.031 %    loss: 0.508600     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.43 %    Acc: 6.985 %    loss: 0.505010     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.45 %    Acc: 6.944 %    loss: 0.502085     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.48 %    Acc: 6.579 %    loss: 0.499735     Sample time: 0.002s    ETA: 0:7s    .\r",
      "Progress: 0.51 %    Acc: 6.562 %    loss: 0.497324     Sample time: 0.002s    ETA: 0:7s    .\r",
      "Progress: 0.53 %    Acc: 6.845 %    loss: 0.495476     Sample time: 0.001s    ETA: 0:5s    .\r",
      "Progress: 0.56 %    Acc: 6.818 %    loss: 0.493422     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.59 %    Acc: 6.793 %    loss: 0.491633     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.61 %    Acc: 7.292 %    loss: 0.489766     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.64 %    Acc: 7.250 %    loss: 0.488521     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.67 %    Acc: 7.212 %    loss: 0.487110     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.69 %    Acc: 7.176 %    loss: 0.485943     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.72 %    Acc: 7.143 %    loss: 0.484717     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.75 %    Acc: 7.112 %    loss: 0.483580     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.77 %    Acc: 7.708 %    loss: 0.482420     Sample time: 0.002s    ETA: 0:5s    .\r",
      "Progress: 0.80 %    Acc: 7.661 %    loss: 0.481474     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.83 %    Acc: 7.812 %    loss: 0.480630     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.85 %    Acc: 7.955 %    loss: 0.479693     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.88 %    Acc: 7.904 %    loss: 0.478846     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.91 %    Acc: 7.679 %    loss: 0.478286     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.93 %    Acc: 7.986 %    loss: 0.477483     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.96 %    Acc: 8.277 %    loss: 0.476751     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 0.99 %    Acc: 8.388 %    loss: 0.476004     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.01 %    Acc: 8.333 %    loss: 0.475507     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.04 %    Acc: 8.125 %    loss: 0.474952     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.07 %    Acc: 8.232 %    loss: 0.474263     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.09 %    Acc: 8.333 %    loss: 0.473789     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.12 %    Acc: 8.140 %    loss: 0.473280     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.15 %    Acc: 8.239 %    loss: 0.472862     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.17 %    Acc: 8.194 %    loss: 0.472488     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.20 %    Acc: 8.016 %    loss: 0.472012     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.23 %    Acc: 8.245 %    loss: 0.471666     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.25 %    Acc: 8.464 %    loss: 0.471083     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.28 %    Acc: 8.418 %    loss: 0.470935     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.31 %    Acc: 8.375 %    loss: 0.470564     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.33 %    Acc: 8.333 %    loss: 0.470213     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.36 %    Acc: 8.293 %    loss: 0.469928     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.39 %    Acc: 8.373 %    loss: 0.469625     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.41 %    Acc: 8.333 %    loss: 0.469278     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.44 %    Acc: 8.295 %    loss: 0.468881     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.47 %    Acc: 8.371 %    loss: 0.468678     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.49 %    Acc: 8.443 %    loss: 0.468380     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.52 %    Acc: 8.513 %    loss: 0.468025     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.55 %    Acc: 8.475 %    loss: 0.467702     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.57 %    Acc: 8.438 %    loss: 0.467540     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.60 %    Acc: 8.299 %    loss: 0.467255     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.63 %    Acc: 8.367 %    loss: 0.466967     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.65 %    Acc: 8.433 %    loss: 0.466617     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.68 %    Acc: 8.398 %    loss: 0.466332     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.71 %    Acc: 8.462 %    loss: 0.466060     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.73 %    Acc: 8.523 %    loss: 0.465834     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.76 %    Acc: 8.396 %    loss: 0.465698     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.79 %    Acc: 8.364 %    loss: 0.465456     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.81 %    Acc: 8.514 %    loss: 0.465343     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.84 %    Acc: 8.571 %    loss: 0.465132     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.87 %    Acc: 8.627 %    loss: 0.464908     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.89 %    Acc: 8.767 %    loss: 0.464661     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.92 %    Acc: 8.733 %    loss: 0.464523     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.95 %    Acc: 8.699 %    loss: 0.464302     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 1.97 %    Acc: 8.667 %    loss: 0.464180     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.00 %    Acc: 8.635 %    loss: 0.463965     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.03 %    Acc: 8.685 %    loss: 0.463799     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.05 %    Acc: 8.574 %    loss: 0.463608     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.08 %    Acc: 8.703 %    loss: 0.463355     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.11 %    Acc: 8.828 %    loss: 0.463221     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.13 %    Acc: 8.873 %    loss: 0.463087     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.16 %    Acc: 8.841 %    loss: 0.462899     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.19 %    Acc: 8.886 %    loss: 0.462773     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.21 %    Acc: 8.780 %    loss: 0.462693     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.24 %    Acc: 8.897 %    loss: 0.462545     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.27 %    Acc: 8.866 %    loss: 0.462415     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.29 %    Acc: 8.980 %    loss: 0.462269     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.32 %    Acc: 9.091 %    loss: 0.462123     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.35 %    Acc: 9.199 %    loss: 0.461925     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.37 %    Acc: 9.167 %    loss: 0.461869     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.40 %    Acc: 9.135 %    loss: 0.461691     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.43 %    Acc: 9.239 %    loss: 0.461604     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.45 %    Acc: 9.207 %    loss: 0.461537     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.48 %    Acc: 9.176 %    loss: 0.461468     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.51 %    Acc: 9.342 %    loss: 0.461284     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.53 %    Acc: 9.310 %    loss: 0.461204     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.56 %    Acc: 9.343 %    loss: 0.461095     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.59 %    Acc: 9.439 %    loss: 0.461004     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.61 %    Acc: 9.659 %    loss: 0.460808     Sample time: 0.002s    ETA: 0:6s    .\r",
      "Progress: 2.64 %    Acc: 9.688 %    loss: 0.460713     Sample time: 0.022s    ETA: 1:18s    .\r",
      "Progress: 2.67 %    Acc: 9.777 %    loss: 0.460551     Sample time: 0.004s    ETA: 0:13s    .\r",
      "Progress: 2.69 %    Acc: 9.865 %    loss: 0.460492     Sample time: 0.002s    ETA: 0:7s    .\r",
      "Progress: 2.72 %    Acc: 9.891 %    loss: 0.460372     Sample time: 0.003s    ETA: 0:11s    .\r",
      "Progress: 2.75 %    Acc: 9.856 %    loss: 0.460338     Sample time: 0.002s    ETA: 0:7s    .\r",
      "Progress: 2.77 %    Acc: 9.940 %    loss: 0.460212     Sample time: 0.002s    ETA: 0:6s    .\r",
      "Progress: 2.80 %    Acc: 10.083 %    loss: 0.460046     Sample time: 0.003s    ETA: 0:10s    .\r",
      "Progress: 2.83 %    Acc: 10.105 %    loss: 0.459967     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.85 %    Acc: 10.185 %    loss: 0.459807     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.88 %    Acc: 10.206 %    loss: 0.459771     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.91 %    Acc: 10.114 %    loss: 0.459752     Sample time: 0.003s    ETA: 0:9s    .\r",
      "Progress: 2.93 %    Acc: 10.079 %    loss: 0.459623     Sample time: 0.002s    ETA: 0:6s    .\r",
      "Progress: 2.96 %    Acc: 10.045 %    loss: 0.459539     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 2.99 %    Acc: 10.122 %    loss: 0.459454     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.01 %    Acc: 10.033 %    loss: 0.459487     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.04 %    Acc: 10.000 %    loss: 0.459454     Sample time: 0.001s    ETA: 0:5s    .\r",
      "Progress: 3.07 %    Acc: 10.022 %    loss: 0.459361     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.09 %    Acc: 10.043 %    loss: 0.459274     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.12 %    Acc: 9.958 %    loss: 0.459233     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.15 %    Acc: 9.874 %    loss: 0.459204     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.17 %    Acc: 9.844 %    loss: 0.459153     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.20 %    Acc: 9.814 %    loss: 0.459101     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.23 %    Acc: 9.734 %    loss: 0.459029     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.25 %    Acc: 9.705 %    loss: 0.458969     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.28 %    Acc: 9.728 %    loss: 0.458933     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.31 %    Acc: 9.700 %    loss: 0.458904     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.33 %    Acc: 9.722 %    loss: 0.458844     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.36 %    Acc: 9.744 %    loss: 0.458751     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.39 %    Acc: 9.717 %    loss: 0.458714     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.41 %    Acc: 9.690 %    loss: 0.458649     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.44 %    Acc: 9.663 %    loss: 0.458604     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.47 %    Acc: 9.685 %    loss: 0.458527     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.49 %    Acc: 9.612 %    loss: 0.458487     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.52 %    Acc: 9.586 %    loss: 0.458434     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.55 %    Acc: 9.655 %    loss: 0.458372     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.57 %    Acc: 9.583 %    loss: 0.458318     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.60 %    Acc: 9.651 %    loss: 0.458272     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.63 %    Acc: 9.672 %    loss: 0.458232     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.65 %    Acc: 9.692 %    loss: 0.458175     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.68 %    Acc: 9.712 %    loss: 0.458128     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.71 %    Acc: 9.732 %    loss: 0.458127     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.73 %    Acc: 9.752 %    loss: 0.458096     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.76 %    Acc: 9.727 %    loss: 0.458061     Sample time: 0.002s    ETA: 0:6s    .\r",
      "Progress: 3.79 %    Acc: 9.703 %    loss: 0.457998     Sample time: 0.016s    ETA: 0:59s    .\r",
      "Progress: 3.81 %    Acc: 9.635 %    loss: 0.457983     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.84 %    Acc: 9.612 %    loss: 0.457951     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.87 %    Acc: 9.546 %    loss: 0.457918     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.89 %    Acc: 9.651 %    loss: 0.457871     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.92 %    Acc: 9.671 %    loss: 0.457828     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.95 %    Acc: 9.773 %    loss: 0.457744     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 3.97 %    Acc: 9.792 %    loss: 0.457693     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.00 %    Acc: 9.851 %    loss: 0.457605     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.03 %    Acc: 9.910 %    loss: 0.457570     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.05 %    Acc: 9.886 %    loss: 0.457513     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.08 %    Acc: 9.943 %    loss: 0.457410     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.11 %    Acc: 10.000 %    loss: 0.457387     Sample time: 0.003s    ETA: 0:11s    .\r",
      "Progress: 4.13 %    Acc: 9.936 %    loss: 0.457384     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.16 %    Acc: 10.032 %    loss: 0.457268     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.19 %    Acc: 10.047 %    loss: 0.457261     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.21 %    Acc: 10.024 %    loss: 0.457245     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.24 %    Acc: 10.000 %    loss: 0.457212     Sample time: 0.002s    ETA: 0:6s    .\r",
      "Progress: 4.27 %    Acc: 9.977 %    loss: 0.457159     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.29 %    Acc: 10.031 %    loss: 0.457122     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.32 %    Acc: 10.046 %    loss: 0.457083     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.35 %    Acc: 10.099 %    loss: 0.457019     Sample time: 0.002s    ETA: 0:8s    .\r",
      "Progress: 4.37 %    Acc: 10.076 %    loss: 0.456991     Sample time: 0.002s    ETA: 0:6s    .\r",
      "Progress: 4.40 %    Acc: 10.053 %    loss: 0.456923     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.43 %    Acc: 10.030 %    loss: 0.456917     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.45 %    Acc: 10.045 %    loss: 0.456887     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.48 %    Acc: 10.096 %    loss: 0.456838     Sample time: 0.002s    ETA: 0:8s    .\r",
      "Progress: 4.51 %    Acc: 10.074 %    loss: 0.456822     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.53 %    Acc: 10.124 %    loss: 0.456764     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.56 %    Acc: 10.211 %    loss: 0.456695     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.59 %    Acc: 10.188 %    loss: 0.456675     Sample time: 0.003s    ETA: 0:9s    .\r",
      "Progress: 4.61 %    Acc: 10.129 %    loss: 0.456658     Sample time: 0.002s    ETA: 0:7s    .\r",
      "Progress: 4.64 %    Acc: 10.143 %    loss: 0.456630     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.67 %    Acc: 10.156 %    loss: 0.456608     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.69 %    Acc: 10.169 %    loss: 0.456588     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.72 %    Acc: 10.253 %    loss: 0.456520     Sample time: 0.003s    ETA: 0:9s    .\r",
      "Progress: 4.75 %    Acc: 10.265 %    loss: 0.456435     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.77 %    Acc: 10.278 %    loss: 0.456431     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.80 %    Acc: 10.256 %    loss: 0.456437     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.83 %    Acc: 10.268 %    loss: 0.456400     Sample time: 0.003s    ETA: 0:10s    .\r",
      "Progress: 4.85 %    Acc: 10.246 %    loss: 0.456406     Sample time: 0.002s    ETA: 0:6s    .\r",
      "Progress: 4.88 %    Acc: 10.190 %    loss: 0.456405     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.91 %    Acc: 10.169 %    loss: 0.456402     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.93 %    Acc: 10.181 %    loss: 0.456377     Sample time: 0.001s    ETA: 0:4s    .\r",
      "Progress: 4.96 %    Acc: 10.160 %    loss: 0.456380     Sample time: 0.003s    ETA: 0:9s    ."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.00 %    Acc: 21.029 %    loss: 0.412667     Sample time: 0.001s    ETA: 0:0s    .\n",
      "Epoch time: 0:8s\n",
      "Validation Acc: 73.020 %        Val loss: 0.20810102\n",
      "EPOCH: 2 / 3\n",
      "Progress: 100.00 %    Acc: 88.621 %    loss: 0.091433     Sample time: 0.003s    ETA: 0:0s    .\n",
      "Epoch time: 0:12s\n",
      "Validation Acc: 93.020 %        Val loss: 0.05475213\n",
      "EPOCH: 3 / 3\n",
      "Progress: 100.00 %    Acc: 94.440 %    loss: 0.045392     Sample time: 0.001s    ETA: 0:0s    .\n",
      "Epoch time: 0:11s\n",
      "Validation Acc: 95.170 %        Val loss: 0.03959225\n"
     ]
    }
   ],
   "source": [
    "epochs=3\n",
    "for epoch in range(epochs):\n",
    "    print(\"EPOCH:\",epoch+1,'/',epochs)\n",
    "    st_tm=time()\n",
    "    run()\n",
    "    print(\"Epoch time: {}:{}s\".format(int(time()-st_tm)//60,int(time()-st_tm)%60))\n",
    "    validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
